{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import time \n",
    "domain_name = 'https://www.dice.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.dice.com/jobs/q-Frontend-l--radius-30-startPage-1-jobs?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input you want\n",
    "Key_work = 'Frontend'\n",
    "Key_location =''\n",
    "key_main=domain_name+'/jobs/'+'q-'+Key_work+'-l-'+Key_location+'-radius-30-startPage-1-jobs?'\n",
    "key_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total have 130 pages\n"
     ]
    }
   ],
   "source": [
    "## Calculate Job Pages\n",
    "response_main = requests.get(key_main)\n",
    "soup = BeautifulSoup(response_main.text,'lxml')\n",
    "pages = int(str(soup.find(\"\",{\"id\":\"posiCountId\"}).text).replace(',',''))//30\n",
    "print('Total have '+str(pages)+' pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 30 is finished\n",
      "page 60 is finished\n",
      "page 90 is finished\n",
      "page 120 is finished\n",
      "Total have :  3600\n"
     ]
    }
   ],
   "source": [
    "## Add all pages's work\n",
    "herf = []\n",
    "page=0\n",
    "for i in range(pages-10):\n",
    "    page=i+1\n",
    "    response_page = requests.get(domain_name+'/jobs/'+'q-'+Key_work+'-l-'+Key_location+'-radius-30-startPage-'+str(page)+'-jobs?')\n",
    "    soup1 = BeautifulSoup(response_page.text,'lxml')\n",
    "    # every page contains 30 urls\n",
    "    for i in range(30):\n",
    "        tmp = domain_name + soup1.find(\"\",{\"id\":\"position\"+str(i)}).get('href')\n",
    "        herf.append(tmp)\n",
    "        \n",
    "    if page%30==0:\n",
    "        print('page '+ str(page)+' is finished')\n",
    "                                \n",
    "print(\"Total have : \",len(herf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_content(url):\n",
    "    judege  = True\n",
    "    \n",
    "    response_work = requests.get(url)\n",
    "    soup = BeautifulSoup(response_work.text,'lxml')\n",
    "    if soup.find(\"\",{\"class\":\"pull-left h1 jobs-page-header-h1\"}):\n",
    "        judege = False\n",
    "    if soup.find(\"\",{\"class\":\"col-md-12 error-page-header\"}):\n",
    "        judege = False\n",
    "    #except AttributeError:\n",
    "        #judege = True\n",
    "        #judege1 = True\n",
    "        \n",
    "    #print(judege)\n",
    "    #print(judege1)\n",
    "    \n",
    "    if judege==True:\n",
    "        try:\n",
    "            \n",
    "            jobTitle = soup.find(\"\",{\"class\":\"jobTitle\"}).text\n",
    "            \n",
    "            jobEmployer = soup.find(\"\",{\"class\":\"employer\"}).text.replace('\\n','').replace('\\t','')\n",
    "            jobLocation = soup.find(\"\",{\"class\":\"location\"}).text.replace('\\n','')\n",
    "            jobPostTime = soup.find(\"\",{\"class\":\"posted hidden-xs\"}).text\n",
    "            jobID = soup.find(\"\",{\"class\":\"company-header-info\"}).text\n",
    "            jobID = jobID.replace('\\n','').split(':')[2].strip().replace('-','')\n",
    "            ## tag job-info is a array .\n",
    "            foo = soup.find_all(\"\",{\"class\":\"row job-info\"})\n",
    "\n",
    "            out = []\n",
    "            for o in foo:\n",
    "                out.append(o.text.replace('\\n','').replace('\\t',''))\n",
    "\n",
    "        except AttributeError:\n",
    "            jobTitle='None'\n",
    "            jobEmployer='None'\n",
    "            jobLocation='None'\n",
    "            jobPostTime='None'\n",
    "            foo='None'\n",
    "            jobID='None'\n",
    "            #jobDescription=''   \n",
    "\n",
    "        try:\n",
    "            jobskillss = out[0]\n",
    "            jobemploymentType = out[1]\n",
    "            jobbaseSalary = out[2]\n",
    "\n",
    "        except IndexError:\n",
    "            jobskillss = 'None'\n",
    "            jobemploymentType = 'None'\n",
    "            jobbaseSalary = 'None'\n",
    "\n",
    "    ## Jobdescription\n",
    "        str1 = str(soup.find(\"\",{\"id\":\"jobdescSec\"}))\n",
    "        soup = BeautifulSoup(str1.replace('<br/>','\\n').replace('</li>','\\n').replace('</strong>','\\n').replace('</p>','\\n').replace('<p>','\\n'),'lxml')\n",
    "        jobDescription = soup.text\n",
    "        joburl = url\n",
    "    else:\n",
    "        return\n",
    "        \n",
    "    \n",
    "    return((jobTitle,jobEmployer,jobLocation,jobPostTime,jobID,jobskillss,jobemploymentType,jobbaseSalary,joburl,jobDescription))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_json(content_arry):\n",
    "    flag=True\n",
    "    for i in content_arry:\n",
    "        if i:\n",
    "            jobTitle = i[0]\n",
    "            jobEmployer = i[1]\n",
    "            jobLocation = i[2]\n",
    "            jobPostTime = i[3]\n",
    "            jobID = i[4]\n",
    "            jobskills = i[5]\n",
    "            jobemploymentType = i[6]\n",
    "            jobbaseSalary = i[7]\n",
    "            joburl = i[8]\n",
    "            jobDescription = i[9]\n",
    "        \n",
    "            if flag==True:\n",
    "                data = {Key_work:[{\n",
    "                             \"jobID\":jobID,\n",
    "                             \"jobTitle\":jobTitle,\n",
    "                             \"jobEmployer\":jobEmployer,\n",
    "                             \"jobLocation\":jobLocation,\n",
    "                             \"jobPostTime\":jobPostTime,\n",
    "                             \"jobskills\":jobskills,\n",
    "                             \"jobemploymentType\":jobemploymentType,\n",
    "                             \"jobbaseSalary\":jobbaseSalary,\n",
    "                             \"joburl\":joburl,\n",
    "                             \"jobDescription\":jobDescription,\n",
    "                                 }]}\n",
    "                flag=False\n",
    "\n",
    "\n",
    "            else:\n",
    "                add_data = {    \"jobID\":jobID,  \n",
    "                                \"jobTitle\":jobTitle,\n",
    "                                \"jobEmployer\":jobEmployer,\n",
    "                                \"jobLocation\":jobLocation,\n",
    "                                \"jobPostTime\":jobPostTime,\n",
    "                                \"jobskills\":jobskills,\n",
    "                                \"jobemploymentType\":jobemploymentType,\n",
    "                                \"jobbaseSalary\":jobbaseSalary,\n",
    "                                \"joburl\":joburl,\n",
    "                                \"jobDescription\":jobDescription  }\n",
    "                data[Key_work].append(add_data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.24622106552124\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "#post_links = herf\n",
    "post_links = herf\n",
    "start = time.time()\n",
    "with Pool(processes=30) as pool:\n",
    "    contents = pool.map(fetch_article_content, post_links)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_to_json(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3599"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[Key_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#過濾條件#\n",
    "\n",
    "#rules = ['android']\n",
    "#rules = ['backend','software engineer','back-end']\n",
    "rules = ['frontend','front-end','UI','web design','web designer','front end']\n",
    "#rules = ['system analyst','systems analyst']\n",
    "#rules = ['network security','security engineer','systems security']\n",
    "#rules = ['project management']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {Key_work:[]}\n",
    "for i in range(len(result[Key_work])):\n",
    "    jobtitle = result[Key_work][i]['jobTitle'].lower().replace('/',' ').replace(')','').replace('(','').replace(',','')\n",
    "    #print(jobtitle)\n",
    "    for rule in rules:\n",
    "        if rule in jobtitle:\n",
    "            data[Key_work].append(result[Key_work][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to json is ok\n"
     ]
    }
   ],
   "source": [
    "with open(Key_work+\".json\",\"w\") as f:\n",
    "    json.dump(data,f)\n",
    "    print('Save to json is ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Crawer_dice.com.faster.ipynb to script\n",
      "[NbConvertApp] Writing 5564 bytes to Crawer_dice.com.faster.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Crawer_dice.com.faster.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
